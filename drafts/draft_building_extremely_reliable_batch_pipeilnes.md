
## Extremely Reliable and Fault Tolerant Batch Inference Pipelines

Batch inference pipelines are trusted to perform the majority of predictions in a large company. In Amazon, an internal MLOps tool vendor calculated over 90% of his tool's users deployed batch pipelines, rather than live model endpoints. Note, this doesn't mean all predictions were used for offline analysis. MAny times, batch pipelines serve online use cases simply because they need. 

### Alert, Identify

### Start, Pause and Continue 

### Skip Records

### Preserve Processing Order: Enqueue entire pipeline starts


### Signaling: Starts and Ends



